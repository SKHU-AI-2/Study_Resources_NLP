{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "input: Write a positive review about a movie.\n",
        "output: I absolutely loved the movie! It was full of amazing performances and the storyline was captivating.\n",
        "\n",
        "input: Write a negative review about a movie.\n",
        "output: Unfortunately, the movie failed to meet my expectations. The plot was predictable and it lacked originality.\n"
      ],
      "metadata": {
        "id": "MKH_iCVd8gx1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gU8-0yT48fFR"
      },
      "outputs": [],
      "source": [
        "from transformers import LlamaForCausalLM, LlamaTokenizer, TextDataset, DataCollatorForLanguageModeling\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# 모델 및 토크나이저 초기화\n",
        "model_name = \"allenai/llama2-7b\"\n",
        "model = LlamaForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# 데이터셋 준비\n",
        "train_file = 'path_to_your_train.txt'  # 학습 데이터 파일 경로\n",
        "train_dataset = TextDataset(\n",
        "    tokenizer=tokenizer,\n",
        "    file_path=train_file,\n",
        "    block_size=128  # 토큰의 최대 길이\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer,\n",
        "    mlm=False  # Masked Language Model을 사용하지 않음\n",
        ")\n",
        "\n",
        "# TrainingArguments 설정\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./llama_finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=5,  # 에포크 수\n",
        "    per_device_train_batch_size=4,  # 배치 크기\n",
        "    save_steps=10_000,\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Trainer 객체 생성 및 학습 시작\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for question_list in tqdm(test['질문']) :\n",
        "    conversation = [ {'role': 'system', 'content': \"요약해서 간략하게 3문장 내외로 대답합니다.\"},\n",
        "        {'role':'user', 'content' : question_list} ] #{'role': 'system', 'content': \"It's a chatbot that only answers in Korean.\"},\n",
        "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, use_cache=True, max_length=400)\n",
        "    preds.append(outputs)"
      ],
      "metadata": {
        "id": "wQeurro48jje"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}